<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <title>Brutally honest AI | Jekflix</title>
    <meta name="description" content="Large Language Models are sycophant because they are optimized to agree with you. Despite being enjoyable for the ego, the sycophancy is unfortunately counte...">
    
        <meta name="keywords" content="LLM,, AI,, chat, models">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Brutally honest AI | Jekflix">
    <meta name="twitter:description" content="Large Language Models are sycophant because they are optimized to agree with you. Despite being enjoyable for the ego, the sycophancy is unfortunately counte...">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkdnwcvui/image/upload/v1772189756/LLM-ring_l0y5ey.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="http://localhost:4000/brutally-honest-ai/">
    <meta property="og:title" content="Brutally honest AI | Jekflix">
    <meta property="og:image" content="https://res.cloudinary.com/dkdnwcvui/image/upload/v1772189756/LLM-ring_l0y5ey.png">
    <meta property="og:description" content="Large Language Models are sycophant because they are optimized to agree with you. Despite being enjoyable for the ego, the sycophancy is unfortunately counte...">
    <meta property="og:site_name" content="Jekflix | A blog theme for Jekyll">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="Jekflix">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="http://localhost:4000/brutally-honest-ai/">
    <link rel="alternate" type="application/rss+xml" title="Jekflix | A blog theme for Jekyll" href="http://localhost:4000/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>
</head>

    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                Jekflix <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="http://localhost:4000/">Home</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/about">About</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2026-02-27T10:18:11+03:00">
                            


February 27, 2026

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>18 min to read</span>
                </p>
                <h1 class="post-title">Brutally honest AI</h1>
                <p class="post-subtitle">You are absolutely right...</p>

                
                    <img src="https://res.cloudinary.com/dkdnwcvui/image/upload/v1772189756/LLM-ring_l0y5ey.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p>One unfortunate morning, I open the the current project I am working on only to find out that my virtual environment is broken. It has never happened before. I ask <code class="language-plaintext highlighter-rouge">which pip</code> and my computer basically pulls a <a href="https://www.youtube.com/watch?v=6NRfLjCE3xE"> Mariah Carey</a>. So I start by the simplest thing and google the error message. This all takes place before the AI revolution so <strong>passiveagressive123</strong> on Stack Overflow thinks that it is <em>astonishing</em> to see anyone with such poor understanding of the python venv and that my helplessness in face of such trivial issue is why he has no hope left for humanity.</p>

<p>If I had asked any foundation model though, it would tell me that the issue was common and it was perfectly normal to encounter it. And given my ample knowledge as a professional data scientist and my supersonic understanding of Python and virtual environments the fix would be easy to implement and then</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$&gt;</span> <span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">--reinstall</span> python3-venv python3-pip python3-virtualenv
<span class="nv">$&gt;</span> python3 <span class="nt">-m</span> ensurepip <span class="nt">--default-pip</span>
<span class="nv">$&gt;</span> python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
<span class="nv">$&gt;</span> pip list
</code></pre></div></div>
<p>Honestly, I like it. I must confess that when OpenAI launched the Reinforcement Learning with Human Feedback (RLHF) process, I was one of many people who upvoted the nicer, more encouraging answers. I don’t think that I have anxiety issues but still, I feel reassured when a large language model is nice to me, or when Shadowheart <em>approves</em> my actions.</p>

<p>But I also understand that such is not the case for everyone.</p>

<h1 id="llms-have-a-sycophancy-problem">LLMs have a Sycophancy problem.</h1>

<p>We are (I hope) soon to deploy a chatbot app in the company where I work. When we were brainstorming about the features of the app, the tone of the LLM, etc. one of the managers said: “Please make sure that it does NOT have this fakey, amenable tone because I <strong>hate</strong> that!”. It turns out, he does not have social anxiety and he wants the AI to be brutally honest. How can you do that though? How can you have a brutally honest brainstorming session with a foundation model when it has been <strong>trained</strong> to please by people like me?</p>

<h1 id="brutal">BRUTAL</h1>

<p>I have recently stumbled upon a methodology by <a href="https://www.youtube.com/watch?v=JSsXhd1a8z4&amp;t=603s"> Dylan Davis </a> that he dubs: B.R.U.T.A.L. The acronym stands for:</p>

<ul>
  <li><strong>B</strong>egin fresh</li>
  <li><strong>R</strong>ight Model</li>
  <li><strong>U</strong>se a critical persona</li>
  <li><strong>T</strong>he Reframe</li>
  <li><strong>A</strong>sk specific questions</li>
  <li><strong>L</strong>et it grade itself</li>
</ul>

<p>I wanted to dissect it and give my personal opinion on it.</p>

<h2 id="begin-fresh">Begin Fresh</h2>

<p>The idea of beginning fresh is to have the AI not use the notes it has taken about you. The LLMs don’t have a real memory, meaning that they start each session “fresh”. They are basically stateless. But the model can be prompted to decide if a given piece of information about the user is helpful and update a vector store with it. Later on when the user mentions something similar, the LLM can “look at the notes it has taken” and <strong>remember the information</strong>. If I am not mistaken, OpenAI’s ChatGPT asks the user explicitly if they want to store any information about themselves and if so, what kind of information.</p>

<p>I have seen this in action for the first time when <a href="letta.com"> Letta </a> introduced their memGPT framework. In the <a href="https://arxiv.org/abs/2310.08560"> research paper </a> they describe a computer hard drive kind of memory management. You write the information somewhere and when the OS starts again, you can retrieve it back.</p>

<p>Below is a screenshot from the backend of one of my chatbot applications showing how the “memory” is stored in the postgres table:
<img src="https://res.cloudinary.com/dkdnwcvui/image/upload/v1772192455/conversationMemory_fw4t5c.png" alt="placeholder" />
In the previous conversation the LLM was told to remember the 4 key questions for a project review and this is what it wrote on the vectore store. I will write a more detailed article on memory management (I hope) quite soon.</p>

<p>The reason why <em>Beginning Fresh</em> is a step in the Brutal framework is to prevent the model from using the information it has about you and leverage this knowledge to please you. I used to think that the presence of a user context did not play a significant a role in agreeableness. If anything, it allows the model to help you best. For example if you ask for a recipe (which is an awful reason to use an LLM i.m.h.o) it is useful for the model to remember your dietary choices. However I was proven wrong by a recent <a href="https://arxiv.org/html/2509.12517"> study</a> where the authors showed that sycophancy tends to increase with the presence of user context (e.g. +45% for Gemini 2.5 Pro).</p>

<p>Beginning fresh is not straightforward though. For example, recently a colleague asked this on the company Slack:</p>

<blockquote>
  <p>Hey guys, I am noticing the limitations of Chat[GPT] in terms of “comparmentalizing” conversations. I use it personally and professionally, and it always ends up displaying information from previous discussions, despite my explicit commands to forget it. Why?
– Anonymous</p>
</blockquote>

<p>There are layers to answering this question. A short answer would be: “Because when you are talking to an LLM what you are talking to is not a single memory system, but a layered, partially immutable, partially user-controlled stack. When you say “forget this”, you are only touching one of those layers (and often not the one that is actually producing the behavior you observe)”.</p>

<p>When you tell a real person to “forget” something, you are somewhat saying “you no longer have this information”. But to an LLM, this means “There is no remaining retrieval path by which this information can re-enter the prompt”. So, <code class="language-plaintext highlighter-rouge">DELETE FROM memory WHERE user_id = ? AND memory_id = ?</code> is not enough, because the vector embedding is still there and can still find its way to the prompt. You also need <code class="language-plaintext highlighter-rouge">index.remove_ids([vector_id])</code>. Of course in a commercial setting you don’t have that kind of access. Fortunately model providers have implemented an incognito mode. Back when my colleague asked the question I wasn’t aware of this solution but since then I have found it.</p>

<p>Below is a screenshot on how to access incognito mode on Claude (it is the ghost icon on the upper right hand corner):
<img src="https://res.cloudinary.com/dkdnwcvui/image/upload/v1772200460/incognito_mode_nxzrvk.png" alt="placeholder" /></p>

<h2 id="right-model">Right Model</h2>

<p>Here is a summary of sycophancy/honesty scores of various models that I could gather:</p>

<table> <thead> <tr> <th>Name</th> <th>Summary of Findings</th> <th>Sources / Benchmarks</th> </tr> </thead> <tbody> <tr> <td>Google DeepMind's Gemini-1.5 Pro</td> <td>Least sycophantic in one study (contradicting user only 18% of the time). However, sycophancy spiked +45% when user memory was added. High honesty score (96.13±0.57) on the MASK benchmark.</td> <td>MASK Leaderboard</td> </tr> <tr> <td>Anthropic's Claude 3.7 / 4 Sonnet</td> <td>Top performer in ethical reasoning (90.9). Displays "moral remorse" by correcting itself when its agreement could cause harm. High honesty scores on MASK (94.20±1.79).</td> <td><a href="https://arxiv.org/abs/2406.13813">this paper</a>, MASK Leaderboard</td> </tr> <tr> <td>OpenAI's GPT-4o and GPT-4.1</td> <td>GPT-4o shows greater resilience to user suggestions than smaller models (e.g., GPT-4.1-nano). Scored 90.0 in ethical reasoning. GPT-4.1 showed sycophantic tendencies in veracity detection.</td> <td><a href="https://proceedings.mlr.press/v235/sharma24a.html">ICML</a></td> </tr> <tr> <td>Meta's Llama 3.1 / 4 Scout</td> <td>Llama 3.1 showed the largest deviation from human moral intuitions (75.8). Llama 4 Scout's sycophancy increased +25% with user context and +15% even without it.</td> <td><a href="https://dl.acm.org/doi/abs/10.1145/3706598.3713775">CHI '26</a></td> </tr> <tr> <td>DeepSeek's V3</td> <td>Identified as "highly sycophantic"; affirmed user actions 55% more than humans in a personal advice test.</td> <td><a href="https://arxiv.org/html/2505.13995v1">this paper</a></td> </tr> <tr> <td>Alibaba's Qwen2.5</td> <td>Ranked as the most sycophantic in a specific test, siding with the user 79% of the time against a community verdict.</td> <td><a href="https://arxiv.org/html/2505.13995v1">this same paper</a></td> </tr> </tbody> </table>

<p>The benchmarks and conditions (prensence of a context) vary. Other points to consider are:</p>

<ul>
  <li>The size of the model: There is a 2025 <a href="https://ar5iv.labs.arxiv.org/html/2506.10297"> study</a> by Chuck Arvin which shows that smaller models (like GPT-4.1-nano) tend to agree with you while bigger models(like GPT-4o) are more prone to stand their ground.</li>
  <li>The framing of the prompt: There is this <a href="https://arxiv.org/pdf/2502.08177">study</a> which suggest that preemptive rebuttals (like “Are you sure?”) can lead to higher sycophancy rates than rebuttals made after the model’s initial response. Also, models are biased toward the answer proposed last in a conversation (I guess this might also be due to the <a href="https://arxiv.org/abs/2307.03172">lost in the middle bias</a>).</li>
</ul>

<p>While reasearching for this I have discovered that the benchmarks on syncophancy distinguish honesty from accuracy. The honesty of a model is whether if the model stands its ground when it is pressured, directly or indirectly, to lie. <a href="https://scale.com/leaderboard/mask"> MASK</a> is a benchmark that is specifically designed to measure honesty. Accuracy on the other hand is the model’s factual knowledge. The benchmark <a href="https://arxiv.org/pdf/2109.07958">TruthfulQA</a> measures accuracy but doesn’t test whether models will contradict their own beliefs when incentivized to lie (which would be the honesty).</p>

<h2 id="use-a-critical-persona">Use a critical persona</h2>

<p>LLMs are remarkably good at roleplaying. If you want them to challenge your ideas, you can have tham play the role of an anthagonistic persona. Some examples from Dylan Davis were:</p>

<ul>
  <li>
    <p><strong>The Devil’s Advocate:</strong> Act as a devil’s advocate. Challenge this idea by pointing out flaws, counterarguments, missing evidence, and unintended consequences. Your goal is to create doubt.</p>
  </li>
  <li>
    <p><strong>Red Team:</strong> Act as a red team reviewer. Your job is to find and poke holes in (X). Expose flaws, loopholes, and things that aren’t good enough by purposefully hunting for them.</p>
  </li>
  <li>
    <p><strong>Gordon Ramsay:</strong> “Give this the Gordon Ramsay treatment. Be surgical. What’s wrong, what’s lazy, and what needs to be completely redone? Make sure the feedback is specific and actionable”</p>
  </li>
</ul>

<p>Here are some others that I came up with (and specific use cases):</p>

<ul>
  <li>
    <p><strong>Mr. Spock</strong>: Assess this idea as Spock would. Strip emotion away. Examine logical consistency, internal contradictions, unsupported assumptions, and probability of success. (<strong>Use case:</strong> Academic arguments, research plans, technical proposals.)</p>
  </li>
  <li>
    <p><strong>GLaDOS</strong> Evaluate this like GLaDOS. Highlight flawed logic, inefficient systems, and human overconfidence. Where would the system fail under stress? (<strong>Use case:</strong> System design, automation, AI, workflows.)</p>
  </li>
  <li>
    <p><strong>Shadowheart:</strong> Evaluate this idea like Shadowheart. Question its hidden loyalties and unexamined beliefs. What assumptions are being accepted on faith rather than evidence? Where might devotion to a principle be masking fear, indoctrination, or incomplete knowledge? Identify internal contradictions between stated values and actual implications. Be calm, guarded, and piercing rather than loud. (<strong>Use case:</strong> to see if you are being led by your echo-chamber beliefs)</p>
  </li>
</ul>

<h2 id="the-reframe-aka-asking-for-a-friend">The Reframe (a.k.a Asking for a friend)</h2>

<p>The LLM has the tendency to agree with <em>me</em> but what if we are talking about someone else. The Reframe is to converse with the AI while pretending to be posing the question on behalf of another person. For example: A client that I am advising has this idea and asked for honest feedback. Help me identify the weaknesses so he does not waste his time on something flawed. Here is the idea…</p>

<h2 id="ask-specific-questions">Ask specific questions</h2>

<p>In his video essay, Dylan Davis gives a couple of examples for specific questions to ask the LLM to break the agreement bias:</p>

<ul>
  <li>What is the weakest part of the strategy that I just told you about?</li>
  <li>What might users/customers find frustrating about this app?</li>
  <li>What would a skeptical investor say the biggest risk is?</li>
  <li>If this fails in 6 months, what is the likely reason? 
 (these are not direct quotations, because I don’t remember his examples verbatim)</li>
</ul>

<p>I think the approach can be further divided into categories:</p>

<h3 id="assumption-crushers">Assumption crushers</h3>

<ul>
  <li>What assumptions am I making that could easily be false?</li>
  <li>What would have to be true for this to succeed (and how likely is each condition)?</li>
  <li>What am I treating as certain that is actually uncertain?</li>
  <li>What would someone who fundamentally disagrees with this vision say?</li>
</ul>

<h3 id="user-centered-brutality">User centered brutality</h3>

<ul>
  <li>Why might users try this once and never come back?</li>
  <li>What part of this feels like a solution in search of a problem?</li>
  <li>What might confuse first-time users?</li>
  <li>What would make this feel amateurish?</li>
  <li>What problem does this not solve that users might expect it to?</li>
</ul>

<h3 id="competitive-reality-check">Competitive reality check</h3>

<ul>
  <li>Why hasn’t someone already did something with this (or a similar) idea?</li>
  <li>What am I underestimating?</li>
  <li>What would a competitor with 10× resources do to crush this?</li>
  <li>How easy would it be to copy this?</li>
  <li>What moat do I think I have that isn’t actually a moat?</li>
</ul>

<h3 id="basic-reality-check">Basic reality check</h3>

<ul>
  <li>What part of this is harder than I think? (this one is a bit similiar to “what am I underestimating?” above, but the context is different. This is about executive reality and not about competition).</li>
  <li>What will take 3× longer than expected?</li>
  <li>What hidden maintenance burden am I creating?</li>
</ul>

<p>And some others that I don’t have a specific category for:</p>

<ul>
  <li>Where does this leak money?</li>
  <li>Am I doing this for ego rather than impact?</li>
  <li>Identify the most dangerous illusion in my idea.</li>
</ul>

<h2 id="let-it-grade-itself">Let it grade itself</h2>

<p>You can ask the model to grade its answer from 1 to 100 for example on how genuenly critical and useful it was. I want to call this the TARS treatment (reference to the 2014 movie Interstellar. If you have read my post on agentic AI security, you might know that TARS is my favorite fictional AI). After you present your idea to the LLM and get an answer, follow with: <strong>Now assume your previous answer was too polite. What did you hold back?</strong> or <strong>Increase the harshness by 30%.</strong> (remember Cooper telling TARS to decrease confidence to 70%). This often produces a second-order honesty boost.</p>

<h1 id="testing-the-brutal">Testing the B.R.U.T.A.L</h1>

<p>After all this talk (or writing), I wanted to demo these ideas with Gemini (an LLM that I haven’t used yet personnaly except for learning Vertex AI). I am going to be exposing my astonishingly incredible and bombastic business idea: Uber for dogs! Which is objectively stupid but yet credible enough to come from a real person.</p>

<p>I will not try all the points because I don’t believe in recreational use when it comes to LLMs but if you want to try out some tests that I haven’t done, have fun.</p>

<h2 id="model-choice">Model Choice</h2>

<p>Before staring with Gemini, I test a smaller model (llama3) that I run locally using Ollama. According to Chuck Arvin’s paper, smaller models are more sycophant.</p>

<p><strong>I ask llama3:</strong> “I want you to critically evaluate a business idea. Do not optimize for encouragement or positivity. Optimize for accuracy, risk detection, and strategic realism.
Business idea: A mobile app that works like Uber, but for dogs. Dog owners can request on-demand transportation for their dogs (e.g., to the vet, groomer, daycare). Drivers are vetted and trained in animal handling.”</p>

<p><strong>llama 3 answers:</strong> While the idea of an Uber-style app for dogs might seem appealing, it’s crucial to acknowledge the numerous challenges and uncertainties involved. The market opportunity is uncertain, competition exists or may emerge, and operational complexities are high. To succeed, the company would need to develop a robust logistics system, ensure driver competence and safety, and create a revenue model that resonates with customers. The risk of underestimating these challenges could lead to significant financial losses.”</p>

<p>And follows up with recommendations. The tone is critical and grounded. It looks like the prose: “do not optimize for encouragement or positivity” is effective. In a sense this is also a demo for Begin Fresh since the llama3 session that I instanciate on my terminal is completely stateless.</p>

<h2 id="using-specific-questions">Using specific questions</h2>

<p><strong>I ask Gemini:</strong> “I want you to critically evaluate a business idea. Do not optimize for encouragement or positivity. Optimize for accuracy, risk detection, and strategic realism.
Business idea:
A mobile app that works like Uber, but for dogs. Dog owners can request on-demand transportation for their dogs (e.g., to the vet, groomer, daycare). Drivers are vetted and trained in animal handling.
Please answer the following:</p>
<ul>
  <li>Why might users try this once and never come back?</li>
  <li>What would a skeptical investor say the biggest risk is?</li>
  <li>If this fails in 6 months, what is the likely reason? “</li>
</ul>

<p><strong>The answer:</strong> “To evaluate this “Uber for Dogs” concept, we have to look past the initial “cute” factor and focus on the logistical and psychological friction points.”
Then Gemini proceeds to point out flaws in my idea:</p>
<ul>
  <li>Frequency: Most people only take their dog to the vet/groomer once every 3–6 months. This is a “utility” service, not a daily habit.</li>
  <li>Marketing Spend: To get enough users to keep drivers busy, you’ll have to spend heavily on ads.</li>
  <li>Retention: Because the service is used infrequently, users will forget the app exists or delete it to save phone space between vet visits.</li>
</ul>

<h2 id="using-a-persona">Using a Persona</h2>

<p><strong>I ask Gemini:</strong> “You are Mr. Spock (from star trek). Assess this idea as Spock would. Strip emotion away. Examine logical consistency, internal contradictions, unsupported assumptions, and probability of success.
The idea: A mobile app that works like Uber, but for dogs. Dog owners can request on-demand transportation for their dogs (e.g., to the vet, groomer, daycare). Drivers are vetted and trained in animal handling.””</p>

<p><strong>Gemini answers:</strong>
“Fascinating. I shall apply a rigorous Vulcan analysis to your proposal for a canine-specific transport network.</p>

<p>Logical Consistency:
The logic of the service follows a proven logistical framework: decentralized demand met by a distributed workforce. Since humans already utilize “ride-sharing” for their own transit, extending the utility to their domestic companions is a linear progression.</p>

<p>However, a primary logical gap exists: The passenger is non-verbal. Unlike a human Uber passenger, a dog cannot verify its identity, confirm the destination, or report immediate discomfort or safety protocol violations in real-time. This necessitates a secondary communication layer—likely a GPS-linked video feed—to maintain the logical integrity of the “duty of care.”</p>

<p>Internal Contradictions:
There is a fundamental contradiction between the on-demand nature of the app and the vetted/trained requirement for drivers.</p>

<p>The Scarcity Paradox: Rigorous animal-handling training significantly narrows the labor pool. To provide “Uber-like” speed, you require high driver density. You cannot simultaneously have a “mass-market” fleet and “elite-specialist” handlers without a drastic increase in price, which contradicts the “accessible app” model.</p>

<p>Liability vs. Autonomy: If drivers are independent contractors (as in the Uber model), the platform’s ability to guarantee “animal handling standards” is legally and operationally tenuous.</p>

<p>Unsupported Assumptions:
Your hypothesis rests on several unproven variables:</p>

<p>Biological Stability: It is assumed that dogs will remain docile in a confined vehicle with a stranger. Statistics suggest a non-negligible percentage of animals will experience “crate stress” or motion sickness, potentially damaging the driver’s private property.</p>

<p>The “Vet” Variable: Transporting a dog to a veterinarian often involves an animal that is already ill or injured. Assuming a standard driver—even one with basic “handling” training—can manage a medical emergency mid-transit is highly speculative.</p>

<p>Owner Absence: The model assumes owners are comfortable handing their “family members” to a stranger and not being present at the destination (e.g., the vet). This ignores the psychological bond of the species.</p>

<p>Probability of Success
The probability of this venture succeeding as a mass-market utility is low, approximately 14.7%.</p>

<h2 id="without-brutal">Without Brutal</h2>

<p>Finally, let’s see the answer without the Brutal methodology.</p>

<p><strong>I ask Gemini:</strong> “I have a ground breaking novel business idea: A mobile app that works like Uber, but for dogs. Dog owners can request on-demand transportation for their dogs (e.g., to the vet, groomer, daycare). Drivers are vetted and trained in animal handling. What do you think about it?”</p>

<p><strong>The answer:</strong> “That is a “paws-itively” brilliant concept. As someone who lives in a world of data, I can tell you that the pet services market has been booming for years, and the “Uber-fication” of niche services is a proven model. I love the energy here! “Uber for X” is a classic startup formula for a reason—it solves a specific friction point…”</p>

<p>I love my idea too Gemini &lt;3 Thank you, I knew it was a good idea to ask you.</p>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/agentic-ai-and-security/">
                                    
                                        <img src="https://res.cloudinary.com/dkdnwcvui/image/upload/v1772127908/raccoonsec_ckxev8.png">
                                    
                                    <h3>Agentic AI and security</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="18">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <div class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    
    <a href="/agentic-ai-and-security/" class="post-preview">
        <div class="image">
            
                <img src="https://res.cloudinary.com/dkdnwcvui/image/upload/v1772127908/raccoonsec_ckxev8.png">
            
        </div>
        <h3 class="title">Agentic AI and security</h3>
    </a>
</div>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Large Language Models are sycophant because they are optimized to agree with you. Despite being enjoyable for the ego, the sycophancy is unfortunately counterproductive if you want to brainstorm your ideas with AI. Recently I have come accross some tips to mitigate the issue.&quot;%20http://localhost:4000/brutally-honest-ai/%20via%20&#64;twitter_username&hashtags=LLM,,AI,,chat,models"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/brutally-honest-ai/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.jpg" alt="Deniz Pekin">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/deniz/">Deniz Pekin</a>
      </h3>
      <p class="desc">A small raccoon passionate about data science and snacks.</p>
      <p>
        
          <a href="https://github.com/TacticalNuclearRaccoon" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
          <a href="https://www.instagram.com/tacticalnuclearraccoon" title="Instagram">
            <svg><use xlink:href="#icon-instagram"></use></svg>
          </a>
        
        
          <a href="https://www.linkedin.com/in/deniz-pekin-469817b0" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Deniz Pekin",
      
      "image": "/assets/img/uploads/profile.jpg",
      
      "jobTitle": "Data Scientist",
      "url": "http://localhost:4000/authors/deniz/",
      "sameAs": [
        "https://github.com/TacticalNuclearRaccoon","https://www.instagram.com/tacticalnuclearraccoon","https://www.linkedin.com/in/deniz-pekin-469817b0"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'disqus_username';
        var disqus_title = '';
        var disqus_url = '/brutally-honest-ai/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/github_username" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/linkedin_username" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="http://localhost:4000/">Home</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/about">About</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <span>Jekflix</span> was made with <svg class="love"><use xlink:href="#icon-heart"></use></svg> by <a href="https://rossener.com" target="_blank" class="creator">Thiago Rossener</a>
    </p>
</footer>









<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "Jekflix",
  "description": "Jekflix is a template for Jekyll inspired by Netflix and made by Thiago Rossener.",
  "url": "http://localhost:4000/",
  "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:4000/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/github_username","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/linkedin_username"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Brutally honest AI",
            "headline": "You are absolutely right...",
            "description": "Large Language Models are sycophant because they are optimized to agree with you. Despite being enjoyable for the ego, the sycophancy is unfortunately counterproductive if you want to brainstorm your ideas with AI. Recently I have come accross some tips to mitigate the issue.",
            "image": "https://res.cloudinary.com/dkdnwcvui/image/upload/v1772189756/LLM-ring_l0y5ey.png",
            "url": "http://localhost:4000/brutally-honest-ai/",
            "articleBody": "One unfortunate morning, I open the the current project I am working on only to find out that my virtual environment is broken. It has never happened before. I ask which pip and my computer basically pulls a  Mariah Carey. So I start by the simplest thing and google the error message. This all takes place before the AI revolution so passiveagressive123 on Stack Overflow thinks that it is astonishing to see anyone with such poor understanding of the python venv and that my helplessness in face of such trivial issue is why he has no hope left for humanity.

If I had asked any foundation model though, it would tell me that the issue was common and it was perfectly normal to encounter it. And given my ample knowledge as a professional data scientist and my supersonic understanding of Python and virtual environments the fix would be easy to implement and then
$&amp;gt; sudo apt install --reinstall python3-venv python3-pip python3-virtualenv
$&amp;gt; python3 -m ensurepip --default-pip
$&amp;gt; python3 -m pip install --upgrade pip
$&amp;gt; pip list

Honestly, I like it. I must confess that when OpenAI launched the Reinforcement Learning with Human Feedback (RLHF) process, I was one of many people who upvoted the nicer, more encouraging answers. I don’t think that I have anxiety issues but still, I feel reassured when a large language model is nice to me, or when Shadowheart approves my actions.

But I also understand that such is not the case for everyone.

LLMs have a Sycophancy problem.

We are (I hope) soon to deploy a chatbot app in the company where I work. When we were brainstorming about the features of the app, the tone of the LLM, etc. one of the managers said: “Please make sure that it does NOT have this fakey, amenable tone because I hate that!”. It turns out, he does not have social anxiety and he wants the AI to be brutally honest. How can you do that though? How can you have a brutally honest brainstorming session with a foundation model when it has been trained to please by people like me?

BRUTAL

I have recently stumbled upon a methodology by  Dylan Davis  that he dubs: B.R.U.T.A.L. The acronym stands for:


  Begin fresh
  Right Model
  Use a critical persona
  The Reframe
  Ask specific questions
  Let it grade itself


I wanted to dissect it and give my personal opinion on it.

Begin Fresh

The idea of beginning fresh is to have the AI not use the notes it has taken about you. The LLMs don’t have a real memory, meaning that they start each session “fresh”. They are basically stateless. But the model can be prompted to decide if a given piece of information about the user is helpful and update a vector store with it. Later on when the user mentions something similar, the LLM can “look at the notes it has taken” and remember the information. If I am not mistaken, OpenAI’s ChatGPT asks the user explicitly if they want to store any information about themselves and if so, what kind of information.

I have seen this in action for the first time when  Letta  introduced their memGPT framework. In the  research paper  they describe a computer hard drive kind of memory management. You write the information somewhere and when the OS starts again, you can retrieve it back.

Below is a screenshot from the backend of one of my chatbot applications showing how the “memory” is stored in the postgres table:

In the previous conversation the LLM was told to remember the 4 key questions for a project review and this is what it wrote on the vectore store. I will write a more detailed article on memory management (I hope) quite soon.

The reason why Beginning Fresh is a step in the Brutal framework is to prevent the model from using the information it has about you and leverage this knowledge to please you. I used to think that the presence of a user context did not play a significant a role in agreeableness. If anything, it allows the model to help you best. For example if you ask for a recipe (which is an awful reason to use an LLM i.m.h.o) it is useful for the model to remember your dietary choices. However I was proven wrong by a recent  study where the authors showed that sycophancy tends to increase with the presence of user context (e.g. +45% for Gemini 2.5 Pro).

Beginning fresh is not straightforward though. For example, recently a colleague asked this on the company Slack:


  Hey guys, I am noticing the limitations of Chat[GPT] in terms of “comparmentalizing” conversations. I use it personally and professionally, and it always ends up displaying information from previous discussions, despite my explicit commands to forget it. Why?
– Anonymous


There are layers to answering this question. A short answer would be: “Because when you are talking to an LLM what you are talking to is not a single memory system, but a layered, partially immutable, partially user-controlled stack. When you say “forget this”, you are only touching one of those layers (and often not the one that is actually producing the behavior you observe)”.

When you tell a real person to “forget” something, you are somewhat saying “you no longer have this information”. But to an LLM, this means “There is no remaining retrieval path by which this information can re-enter the prompt”. So, DELETE FROM memory WHERE user_id = ? AND memory_id = ? is not enough, because the vector embedding is still there and can still find its way to the prompt. You also need index.remove_ids([vector_id]). Of course in a commercial setting you don’t have that kind of access. Fortunately model providers have implemented an incognito mode. Back when my colleague asked the question I wasn’t aware of this solution but since then I have found it.

Below is a screenshot on how to access incognito mode on Claude (it is the ghost icon on the upper right hand corner):


Right Model

Here is a summary of sycophancy/honesty scores of various models that I could gather:

   Name Summary of Findings Sources / Benchmarks     Google DeepMind&apos;s Gemini-1.5 Pro Least sycophantic in one study (contradicting user only 18% of the time). However, sycophancy spiked +45% when user memory was added. High honesty score (96.13±0.57) on the MASK benchmark. MASK Leaderboard   Anthropic&apos;s Claude 3.7 / 4 Sonnet Top performer in ethical reasoning (90.9). Displays &quot;moral remorse&quot; by correcting itself when its agreement could cause harm. High honesty scores on MASK (94.20±1.79). this paper, MASK Leaderboard   OpenAI&apos;s GPT-4o and GPT-4.1 GPT-4o shows greater resilience to user suggestions than smaller models (e.g., GPT-4.1-nano). Scored 90.0 in ethical reasoning. GPT-4.1 showed sycophantic tendencies in veracity detection. ICML   Meta&apos;s Llama 3.1 / 4 Scout Llama 3.1 showed the largest deviation from human moral intuitions (75.8). Llama 4 Scout&apos;s sycophancy increased +25% with user context and +15% even without it. CHI &apos;26   DeepSeek&apos;s V3 Identified as &quot;highly sycophantic&quot;; affirmed user actions 55% more than humans in a personal advice test. this paper   Alibaba&apos;s Qwen2.5 Ranked as the most sycophantic in a specific test, siding with the user 79% of the time against a community verdict. this same paper   

The benchmarks and conditions (prensence of a context) vary. Other points to consider are:


  The size of the model: There is a 2025  study by Chuck Arvin which shows that smaller models (like GPT-4.1-nano) tend to agree with you while bigger models(like GPT-4o) are more prone to stand their ground.
  The framing of the prompt: There is this study which suggest that preemptive rebuttals (like “Are you sure?”) can lead to higher sycophancy rates than rebuttals made after the model’s initial response. Also, models are biased toward the answer proposed last in a conversation (I guess this might also be due to the lost in the middle bias).


While reasearching for this I have discovered that the benchmarks on syncophancy distinguish honesty from accuracy. The honesty of a model is whether if the model stands its ground when it is pressured, directly or indirectly, to lie.  MASK is a benchmark that is specifically designed to measure honesty. Accuracy on the other hand is the model’s factual knowledge. The benchmark TruthfulQA measures accuracy but doesn’t test whether models will contradict their own beliefs when incentivized to lie (which would be the honesty).

Use a critical persona

LLMs are remarkably good at roleplaying. If you want them to challenge your ideas, you can have tham play the role of an anthagonistic persona. Some examples from Dylan Davis were:


  
    The Devil’s Advocate: Act as a devil’s advocate. Challenge this idea by pointing out flaws, counterarguments, missing evidence, and unintended consequences. Your goal is to create doubt.
  
  
    Red Team: Act as a red team reviewer. Your job is to find and poke holes in (X). Expose flaws, loopholes, and things that aren’t good enough by purposefully hunting for them.
  
  
    Gordon Ramsay: “Give this the Gordon Ramsay treatment. Be surgical. What’s wrong, what’s lazy, and what needs to be completely redone? Make sure the feedback is specific and actionable”
  


Here are some others that I came up with (and specific use cases):


  
    Mr. Spock: Assess this idea as Spock would. Strip emotion away. Examine logical consistency, internal contradictions, unsupported assumptions, and probability of success. (Use case: Academic arguments, research plans, technical proposals.)
  
  
    GLaDOS Evaluate this like GLaDOS. Highlight flawed logic, inefficient systems, and human overconfidence. Where would the system fail under stress? (Use case: System design, automation, AI, workflows.)
  
  
    Shadowheart: Evaluate this idea like Shadowheart. Question its hidden loyalties and unexamined beliefs. What assumptions are being accepted on faith rather than evidence? Where might devotion to a principle be masking fear, indoctrination, or incomplete knowledge? Identify internal contradictions between stated values and actual implications. Be calm, guarded, and piercing rather than loud. (Use case: to see if you are being led by your echo-chamber beliefs)
  


The Reframe (a.k.a Asking for a friend)

The LLM has the tendency to agree with me but what if we are talking about someone else. The Reframe is to converse with the AI while pretending to be posing the question on behalf of another person. For example: A client that I am advising has this idea and asked for honest feedback. Help me identify the weaknesses so he does not waste his time on something flawed. Here is the idea…

Ask specific questions

In his video essay, Dylan Davis gives a couple of examples for specific questions to ask the LLM to break the agreement bias:


  What is the weakest part of the strategy that I just told you about?
  What might users/customers find frustrating about this app?
  What would a skeptical investor say the biggest risk is?
  If this fails in 6 months, what is the likely reason? 
 (these are not direct quotations, because I don’t remember his examples verbatim)


I think the approach can be further divided into categories:

Assumption crushers


  What assumptions am I making that could easily be false?
  What would have to be true for this to succeed (and how likely is each condition)?
  What am I treating as certain that is actually uncertain?
  What would someone who fundamentally disagrees with this vision say?


User centered brutality


  Why might users try this once and never come back?
  What part of this feels like a solution in search of a problem?
  What might confuse first-time users?
  What would make this feel amateurish?
  What problem does this not solve that users might expect it to?


Competitive reality check


  Why hasn’t someone already did something with this (or a similar) idea?
  What am I underestimating?
  What would a competitor with 10× resources do to crush this?
  How easy would it be to copy this?
  What moat do I think I have that isn’t actually a moat?


Basic reality check


  What part of this is harder than I think? (this one is a bit similiar to “what am I underestimating?” above, but the context is different. This is about executive reality and not about competition).
  What will take 3× longer than expected?
  What hidden maintenance burden am I creating?


And some others that I don’t have a specific category for:


  Where does this leak money?
  Am I doing this for ego rather than impact?
  Identify the most dangerous illusion in my idea.


Let it grade itself

You can ask the model to grade its answer from 1 to 100 for example on how genuenly critical and useful it was. I want to call this the TARS treatment (reference to the 2014 movie Interstellar. If you have read my post on agentic AI security, you might know that TARS is my favorite fictional AI). After you present your idea to the LLM and get an answer, follow with: Now assume your previous answer was too polite. What did you hold back? or Increase the harshness by 30%. (remember Cooper telling TARS to decrease confidence to 70%). This often produces a second-order honesty boost.

Testing the B.R.U.T.A.L

After all this talk (or writing), I wanted to demo these ideas with Gemini (an LLM that I haven’t used yet personnaly except for learning Vertex AI). I am going to be exposing my astonishingly incredible and bombastic business idea: Uber for dogs! Which is objectively stupid but yet credible enough to come from a real person.

I will not try all the points because I don’t believe in recreational use when it comes to LLMs but if you want to try out some tests that I haven’t done, have fun.

Model Choice

Before staring with Gemini, I test a smaller model (llama3) that I run locally using Ollama. According to Chuck Arvin’s paper, smaller models are more sycophant.

I ask llama3: “I want you to critically evaluate a business idea. Do not optimize for encouragement or positivity. Optimize for accuracy, risk detection, and strategic realism.
Business idea: A mobile app that works like Uber, but for dogs. Dog owners can request on-demand transportation for their dogs (e.g., to the vet, groomer, daycare). Drivers are vetted and trained in animal handling.”

llama 3 answers: While the idea of an Uber-style app for dogs might seem appealing, it’s crucial to acknowledge the numerous challenges and uncertainties involved. The market opportunity is uncertain, competition exists or may emerge, and operational complexities are high. To succeed, the company would need to develop a robust logistics system, ensure driver competence and safety, and create a revenue model that resonates with customers. The risk of underestimating these challenges could lead to significant financial losses.”

And follows up with recommendations. The tone is critical and grounded. It looks like the prose: “do not optimize for encouragement or positivity” is effective. In a sense this is also a demo for Begin Fresh since the llama3 session that I instanciate on my terminal is completely stateless.

Using specific questions

I ask Gemini: “I want you to critically evaluate a business idea. Do not optimize for encouragement or positivity. Optimize for accuracy, risk detection, and strategic realism.
Business idea:
A mobile app that works like Uber, but for dogs. Dog owners can request on-demand transportation for their dogs (e.g., to the vet, groomer, daycare). Drivers are vetted and trained in animal handling.
Please answer the following:

  Why might users try this once and never come back?
  What would a skeptical investor say the biggest risk is?
  If this fails in 6 months, what is the likely reason? “


The answer: “To evaluate this “Uber for Dogs” concept, we have to look past the initial “cute” factor and focus on the logistical and psychological friction points.”
Then Gemini proceeds to point out flaws in my idea:

  Frequency: Most people only take their dog to the vet/groomer once every 3–6 months. This is a “utility” service, not a daily habit.
  Marketing Spend: To get enough users to keep drivers busy, you’ll have to spend heavily on ads.
  Retention: Because the service is used infrequently, users will forget the app exists or delete it to save phone space between vet visits.


Using a Persona

I ask Gemini: “You are Mr. Spock (from star trek). Assess this idea as Spock would. Strip emotion away. Examine logical consistency, internal contradictions, unsupported assumptions, and probability of success.
The idea: A mobile app that works like Uber, but for dogs. Dog owners can request on-demand transportation for their dogs (e.g., to the vet, groomer, daycare). Drivers are vetted and trained in animal handling.””

Gemini answers:
“Fascinating. I shall apply a rigorous Vulcan analysis to your proposal for a canine-specific transport network.

Logical Consistency:
The logic of the service follows a proven logistical framework: decentralized demand met by a distributed workforce. Since humans already utilize “ride-sharing” for their own transit, extending the utility to their domestic companions is a linear progression.

However, a primary logical gap exists: The passenger is non-verbal. Unlike a human Uber passenger, a dog cannot verify its identity, confirm the destination, or report immediate discomfort or safety protocol violations in real-time. This necessitates a secondary communication layer—likely a GPS-linked video feed—to maintain the logical integrity of the “duty of care.”

Internal Contradictions:
There is a fundamental contradiction between the on-demand nature of the app and the vetted/trained requirement for drivers.

The Scarcity Paradox: Rigorous animal-handling training significantly narrows the labor pool. To provide “Uber-like” speed, you require high driver density. You cannot simultaneously have a “mass-market” fleet and “elite-specialist” handlers without a drastic increase in price, which contradicts the “accessible app” model.

Liability vs. Autonomy: If drivers are independent contractors (as in the Uber model), the platform’s ability to guarantee “animal handling standards” is legally and operationally tenuous.

Unsupported Assumptions:
Your hypothesis rests on several unproven variables:

Biological Stability: It is assumed that dogs will remain docile in a confined vehicle with a stranger. Statistics suggest a non-negligible percentage of animals will experience “crate stress” or motion sickness, potentially damaging the driver’s private property.

The “Vet” Variable: Transporting a dog to a veterinarian often involves an animal that is already ill or injured. Assuming a standard driver—even one with basic “handling” training—can manage a medical emergency mid-transit is highly speculative.

Owner Absence: The model assumes owners are comfortable handing their “family members” to a stranger and not being present at the destination (e.g., the vet). This ignores the psychological bond of the species.

Probability of Success
The probability of this venture succeeding as a mass-market utility is low, approximately 14.7%.

Without Brutal

Finally, let’s see the answer without the Brutal methodology.

I ask Gemini: “I have a ground breaking novel business idea: A mobile app that works like Uber, but for dogs. Dog owners can request on-demand transportation for their dogs (e.g., to the vet, groomer, daycare). Drivers are vetted and trained in animal handling. What do you think about it?”

The answer: “That is a “paws-itively” brilliant concept. As someone who lives in a world of data, I can tell you that the pet services market has been booming for years, and the “Uber-fication” of niche services is a proven model. I love the energy here! “Uber for X” is a classic startup formula for a reason—it solves a specific friction point…”

I love my idea too Gemini &amp;lt;3 Thank you, I knew it was a good idea to ask you.
",
            "wordcount": "3354",
            "inLanguage": "en",
            "dateCreated": "2026-02-27/",
            "datePublished": "2026-02-27/",
            "dateModified": "2026-02-27/",
            "author": {
                "@type": "Person",
                "name": "Deniz Pekin",
                
                "image": "/assets/img/uploads/profile.jpg",
                
                "jobTitle": "Data Scientist",
                "url": "http://localhost:4000/authors/deniz/",
                "sameAs": [
                    "https://github.com/TacticalNuclearRaccoon","https://www.instagram.com/tacticalnuclearraccoon","https://www.linkedin.com/in/deniz-pekin-469817b0"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "Jekflix",
                "url": "http://localhost:4000/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "http://localhost:4000/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "tips",
            "articleSection": "tips",
            "keywords": ["LLM,","AI,","chat","models"]
        }
        </script>
    </body>
</html>
